# -*- coding: utf-8 -*-
"""Quantitative/Categorical Colab- BTT_brainwave

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bvIdg7rc7Z7Pjj7G8RtRvAuSfntxemoq
"""

import numpy as np
import pandas as pd
import seaborn as sns

import os
import matplotlib.pyplot as plt

import sklearn
from sklearn.svm import SVC
from sklearn.metrics import balanced_accuracy_score, roc_auc_score, accuracy_score, confusion_matrix, roc_curve
from scipy.stats import zscore, pearsonr, uniform
from sklearn.model_selection import KFold, StratifiedKFold, RandomizedSearchCV

from scipy.io import loadmat

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score
from sklearn.metrics import accuracy_score

"""# **Load in Data Frames**

TRAIN_CATEGORICAL.csv      TRAIN_QUANTITATIVE.csv
"""

from google.colab import drive
drive.mount('/content/drive')

# categorical variable dataframe

file_path_trainC = "/content/drive/My Drive/data_test/TRAIN_CATEGORICAL.csv"
df1 = pd.read_csv(file_path_trainC)

train_cat = pd.read_csv(file_path_trainC)
train_cat.head()

train_cat.columns

# Functional Connection Matrices

file_path_trainFCM = "/content/drive/My Drive/data_test/TRAIN_FUNCTIONAL_CONNECTOME_MATRICES.csv"
train_FCM = pd.read_csv(file_path_trainFCM)
train_FCM.head()

train_FCM.columns

print(train_FCM)

# Quantitative variable train dataframe

file_path_trainQ = '/content/drive/My Drive/data_test/TRAIN_QUANTITATIVE.csv'
train_Quant = pd.read_csv(file_path_trainQ)
train_Quant.head()

train_Quant.columns

# ADHD and Sex solutions dataframe for model training

file_path_trainS = '/content/drive/My Drive/data_test/TRAINING_SOLUTIONS.csv'
train_Solutions = pd.read_csv(file_path_trainS)
train_Solutions.head()

train_Solutions.columns

"""# **Exploratory Data Analysis**

Use .info() and .describe() to summarize each dataset.
"""

train_cat.info()

"""Understand the distribution of the categorical variables with .value_counts()."""

# Barratt_Barratt_P2_Occ - Barratt Simplified Measure of Social Status - Parent 2 Occupation
train_cat['Barratt_Barratt_P2_Occ'].value_counts()

# look back at the dictionary to see what category these integers [0, 45, 35] represent.

"""Notice the parent 2 occupation with the most frequency is 0: homemaker, stay at home parent.

**Visualize distributions:**
"""

sns.countplot(x='Barratt_Barratt_P2_Occ', data=train_cat[['Barratt_Barratt_P2_Occ']])
plt.title(f"Distribution of Barratt_Barratt_P2_Occ")
plt.xticks(rotation=45)
plt.show()

# Distribution of MRI_Track_Age_at_Scan
train_Quant['MRI_Track_Age_at_Scan'].hist(figsize=(12, 10), bins=20)
plt.suptitle("MRI_Track_Age_at_Scan Distributions")
plt.xlabel('MRI_Track_Age_at_Scan')
plt.ylabel('Frequency Count')
plt.show()

# Gender distribution
train_Solutions['Sex_F'].value_counts()

train_Solutions['Sex_F'].value_counts().plot(kind='bar', color='blue')
plt.title('Gender Distribution')
plt.xlabel('Gender (0 = Male, 1 = Female)')
plt.ylabel('Count')
plt.show()

# ADHD distribution
train_Solutions['ADHD_Outcome'].value_counts()

train_Solutions['ADHD_Outcome'].value_counts().plot(kind='bar', color='blue')
plt.title('ADHD Outcome')
plt.xlabel('Outcome (0 = No, 1 = Yes)')
plt.ylabel('Count')
plt.show()

"""### Testing Predictiveness"""

train_Quant.columns

# Plot the distribution of the SDQ_SDQ_Emotional_Problems variable
plt.figure(figsize=(8, 6))
sns.histplot(train_Quant['SDQ_SDQ_Emotional_Problems'], kde=True, color='skyblue')
plt.title('Distribution of SDQ_SDQ_Emotional_Problems')
plt.xlabel('SDQ_SDQ_Emotional_Problems')
plt.ylabel('Frequency')
plt.show()

"""This boxplot examines the relationship between `SDQ_SDQ_Emotional_Problems` and `ADHD_outcome` (as a target variable)."""

# Check for correlation with ADHD outcome
train_Quant_copy = train_Quant.copy()
train_Quant_copy['ADHD_Outcome'] = train_Solutions['ADHD_Outcome']

plt.figure(figsize=(8, 6))
sns.boxplot(x='ADHD_Outcome', y='SDQ_SDQ_Emotional_Problems', data=train_Quant_copy)
plt.title('SDQ_SDQ_Emotional_Problems vs ADHD Outcome')
plt.xlabel('ADHD Outcome')
plt.ylabel('SDQ_SDQ_Emotional_Problems')
plt.show()

"""The boxplot reveals that individuals diagnosed with ADHD tend to have a higher median on the SDQ Emotional Problems scale compared to non-ADHD individuals. Additionally, the boxplot for the ADHD group displays greater variability, as evidenced by its extended range. This suggests that emotional problems are not only more pronounced but also more diverse within the ADHD group."""

train_cat.columns

"""Let's look at `Barratt_Barratt_P1_Edu` which indicates the Parent 1 level of education

- 3=Less than 7th grade
- 6=Junior high/Middle school (9th grade)
- 9=Partial high school (10th or 11th grade)
- 12=High school graduate
- 15=Partial college (at least one year)
- 18=College education
- 21=Graduate degree
"""

import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(data=train_cat, x='Barratt_Barratt_P1_Edu', hue=train_Solutions['ADHD_Outcome'])
plt.title('ADHD Prevalence by Parent 1 Education')
plt.show()

train_cat['Barratt_Barratt_P1_Edu'].value_counts()

"""Most of the data points fall into a specific category (e.g., 21 has 470 entries out of a total 1213). This means that even if ADHD prevalence appears higher in this category, it might just reflect that there are more people in this group overall, rather than an actual trend.

To adress this, normalize the data or compute percentages within each category to account for differences in group sizes. Let's compute ADHD percentage for each category.
"""

# Add ADHD_Outcome directly to a copy of the train_cat dataset for grouping
train_cat_copy = train_cat.copy()
train_cat_copy['ADHD_Outcome'] = train_Solutions['ADHD_Outcome']

adhd_percentages = train_cat_copy.groupby('Barratt_Barratt_P1_Edu')['ADHD_Outcome'].mean()
print(adhd_percentages)

"""Categories like 3 - Less than 7th grade (80%) and 12 - High school graduate (72%) show some of the highest proportions of ADHD outcomes. Categories 21 - Graduate degree (67.2%) have relatively lower ADHD proportions compared to middle education levels."""

train_cat['Barratt_Barratt_P1_Edu'].value_counts()

"""### One Hot Encoding"""

for col in train_cat.select_dtypes(include='int').columns:
    train_cat[col] = train_cat[col].astype('category')

# Creating a list of all of the columns except the first
columns_to_encode = train_cat.columns[1:].tolist()

# Print the columns to encode
print("Columns to encode:", columns_to_encode)

# encoding categorical data
train_encoded = pd.get_dummies(train_cat[columns_to_encode], drop_first=True)
train_encoded = train_encoded.applymap(lambda x: 1 if x is True else (0 if x is False else x))

# Combine encoded columns with the rest of the DataFrame
cat_train_final = pd.concat([train_cat.drop(columns=columns_to_encode), train_encoded], axis=1)

# ensure it looks correct
cat_train_final.head()

"""### Train and Test Dataframs"""

# load in test categorical dataframe

# file_path_testC = "/content/drive/My Drive/data_test/TEST_CATEGORICAL.csv"
# test_cat = pd.read_csv(file_path_testC)
# #(test_cat.head()

file_path_testC = "/content/drive/My Drive/data_test/TEST_CATEGORICAL.xlsx"
test_cat = pd.read_excel(file_path_testC)
#test_cat = df.to_csv('TEST_CATEGORICAL.csv')

# convert our int variables to categories
for col in test_cat.select_dtypes(include='int').columns:
    test_cat[col] = test_cat[col].astype('category')

# Encode categorical variables in test
test_encoded = pd.get_dummies(test_cat[columns_to_encode], drop_first=True)
test_encoded = test_encoded.applymap(lambda x: 1 if x is True else (0 if x is False else x))

# # Ensure test_encoded has the same columns as train_encoded
# missing_cols = set(train_encoded.columns) - set(test_encoded.columns)
# for col in missing_cols:
#     test_encoded[col] = 0  # Add missing columns with 0 values

# # Ensure test_encoded columns are in the same order as train_encoded
# test_encoded = test_encoded.reindex(columns=train_encoded.columns, fill_value=0)

# Combine encoded columns with the rest of the DataFrame
cat_test_final = pd.concat([test_cat.drop(columns=columns_to_encode), test_encoded], axis=1)

cat_test_final.head()

"""### Merging Data Frame"""

train_cat_FCM = pd.merge(cat_train_final, train_FCM, on = 'participant_id')

train_df = pd.merge(train_cat_FCM, train_Quant, on = 'participant_id')

# ensure it looks accurate
train_df.head()

"""## Merge test dataframes"""

file_path_testFCM = "/content/drive/My Drive/data_test/TEST_FUNCTIONAL_CONNECTOME_MATRICES.csv"
test_FCM = pd.read_csv(file_path_testFCM)
#print(train_FCM.head())

file_path_testQ = "/content/drive/My Drive/data_test/TEST_QUANTITATIVE_METADATA.xlsx"
test_Quant = pd.read_excel(file_path_testQ)
#print(train_Quant.head())

test_cat_FCM = pd.merge(cat_test_final, test_FCM, on = 'participant_id')

test_df = pd.merge(test_cat_FCM, test_Quant, on = 'participant_id')

# ensure it looks accurate
test_df.head()

"""### NA Values"""

# check how many NA values we have
print(train_df.isna().sum())

# 371 NANs values
# 360 in MRI_Track_age_at_Scan
# 11 in PreInt_Demos_Fam_Child_Ethnicity

train_df.fillna({'MRI_Track_Age_at_Scan':train_df['MRI_Track_Age_at_Scan'].mean()}, inplace = True)
train_df.fillna({'PreInt_Demos_Fam_Child_Ethnicity':train_df['PreInt_Demos_Fam_Child_Ethnicity'].mean()}, inplace = True)

print(train_df.isna().sum().sum()) # should now be zero

train_df.ffill(inplace=True)
print(train_df.isna().sum().sum())

# Fill NAs of test data

for col in test_df.columns:
    if test_df[col].isna().sum() > 0:  # Check if the column has NaN values
        if test_df[col].dtype in ['float64', 'int64']:  # Ensure it's numeric
            test_df[col] = test_df[col].fillna(test_df[col].mean())  # Avoid inplace
        else:
            print(f"Skipping non-numeric column: {col}")

# Identify continuous numeric columns (exclude one-hot encoded/binary columns)
continuous_cols = [
    col for col in train_df.select_dtypes(include=['float64', 'int64']).columns
    if train_df[col].nunique() > 2
]

print("Continuous columns to process:", continuous_cols)

# Outlier Handling: Cap values at the 1st and 99th percentiles
for col in continuous_cols:
    lower_bound = train_df[col].quantile(0.01)
    upper_bound = train_df[col].quantile(0.99)
    train_df[col] = train_df[col].clip(lower_bound, upper_bound)

    # For the test set, use the training set's quantiles to ensure consistency
    if col in test_df.columns:
        test_df[col] = test_df[col].clip(lower_bound, upper_bound)

# Normalization: Scale the continuous features with StandardScaler
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

# Fit the scaler on training continuous columns and transform them
train_df[continuous_cols] = scaler.fit_transform(train_df[continuous_cols])

# Use the same scaler (fitted on train) to transform the test set
test_df[continuous_cols] = scaler.transform(test_df[continuous_cols])

# Summary of the scaled data:
print("After scaling, summary of a continuous feature:")
print(train_df[continuous_cols[0]].describe())